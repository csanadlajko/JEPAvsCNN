{
    "ijepa": {
        "DEPTH": 6,
        "DROP_RATE": 0.1,
        "CHANNELS": 3,
        "EMBED_DIM": 256,
        "IMAGE_SIZE": 128,
        "PATCH_SIZE": 16,
        "MLP_DIM": 512,
        "EPOCHS": 10,
        "NUM_HEADS": 8,
        "BATCH_SIZE": 16,
        "LEARNING_RATE": 0.00005,
        "MOMENTUM": 0.996,
        "NUM_TARGET_BLOCKS": 2,
        "NUM_CLASSES": 10
    },
    "llmjepa": {
        "LEARNING_RATE": 0.01,
        "MOMENTUM": 0.996,
        "PRED_TOKENS": 1,
        "MODEL_NAME": "gpt2",
        "LM_GAMMA": 0.9,
        "JEPA_LAMBDA": 0.1,
        "EPOCHS": 3,
        "BATCH_SIZE": 1
    }
}